{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from scratch (Binary Classification)\n",
    "Implementation of Logistic Regression for Binary Classification using Gradient Descent.\n",
    "\n",
    "#### Sources:\n",
    "\n",
    "[Cost Function and Gradient Descent](https://www.coursera.org/learn/machine-learning/supplement/0hpMl/simplified-cost-function-and-gradient-descent)\n",
    "\n",
    "\n",
    "#### Optimization algorithms:\n",
    "[[advanced-optimization]](https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization)\n",
    "- Gradient descent\n",
    "- Conjugate gradient\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "\n",
    "#### Useful Links:\n",
    "\n",
    "[Logistic regression (binary) - computing the gradient](https://www.youtube.com/watch?v=hWLdFMccpTY)\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "add accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Markdown as md, Image\n",
    "from numpy.random import RandomState\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "prng = RandomState(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression hypothesis function is defined as:\n",
    "\n",
    "$$h_\\theta(x) = g(z),$$\n",
    "\n",
    "where function g is the Sigmoid function and is defined as:\n",
    "\n",
    "$$g(z)= \\frac{1}{1+e^{-z}},$$\n",
    "\n",
    "and \n",
    "\n",
    "$$z = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n = \\theta^TX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(z):\n",
    "    \"\"\"\n",
    "    Applies sigmoid function to an integer or an array\n",
    "    \n",
    "    Params:\n",
    "        z (int or array)\n",
    "    \"\"\"\n",
    "    return 1/(1+(np.exp(-z)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function - Properties\n",
    "[[decision-boundary]](https://www.coursera.org/learn/machine-learning/supplement/N8qsm/decision-boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z = 0 \\implies e^{-z} = 1 \\implies  g(z) \\rightarrow 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z \\rightarrow \\infty, \\implies e^{-z} \\rightarrow 0 \\implies g(z) \\rightarrow 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z \\rightarrow -\\infty, \\implies e^{-z} \\rightarrow 0 \\implies g(z) \\rightarrow 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('z=0, g(z) = {}'.format(sigmoid_function(0)))\n",
    "print('z=20, g(z) = {}'.format(sigmoid_function(20).round(8)))\n",
    "print('z=-20, g(z) = {}'.format(sigmoid_function(-20).round(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorised Cost Function:\n",
    "[[cost-function]](https://www.coursera.org/learn/machine-learning/supplement/bgEt4/cost-function)\n",
    "\n",
    "We will try to minimise the cost function. The vectorised form of Cost Function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{m} (-y^{T}log(h) - (1-y)^{T}log(1-h)),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where h is is the hypothesis function in vectorised form,\n",
    "$$h=g(X\\theta).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x, y, theta):\n",
    "    \"\"\"\n",
    "    Returns the cost for given x, y, and theta.\n",
    "    \n",
    "    Params:\n",
    "        x (numpy array): ((M x (N+1)) matrix), these are \n",
    "            the dataset features.\n",
    "            \n",
    "        y (numpy array): (M x 1) column vector, this is\n",
    "            the value to be predicted.\n",
    "            \n",
    "        theta (numpy array): Co-efficients\n",
    "    \n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    z = x @ theta\n",
    "    h = sigmoid_function(z)\n",
    "    \n",
    "    c1 = -y.T @ np.log(h)\n",
    "    c2 = (1-y).T @ np.log(1-h)\n",
    "    \n",
    "    J = (1/m) * ( c1 - c2 )\n",
    "    \n",
    "    return J[0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "[[gradient-descent]](https://www.coursera.org/learn/machine-learning/supplement/0hpMl/simplified-cost-function-and-gradient-descent)\n",
    "\n",
    "We will use Gradient Descent to minimise the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General form of gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta),$$\n",
    "\n",
    "repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorised implementation after working out the partial derivatives:\n",
    "\n",
    "$$\\theta := \\theta - \\frac{\\alpha}{m} X^T(g(X\\theta) - y)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Returns coefficients (theta) by performing one step\n",
    "    of gradient descent.\n",
    "    \n",
    "    Params:\n",
    "        x (numpy array): ((M x (N+1)) matrix), these are \n",
    "            the dataset features.\n",
    "            \n",
    "        y (numpy array): (M x 1) column vector, this is\n",
    "            the value to be predicted.\n",
    "            \n",
    "        theta (numpy array): Co-efficients\n",
    "        \n",
    "        alpha (float): Learning rate for gradient descent\n",
    "            algorithm\n",
    "    \n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    z = x @ theta\n",
    "    h = sigmoid_function(z)\n",
    "    errors = h - y\n",
    "    theta = theta - ((alpha/m) * (x.T @ errors))\n",
    "    return theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y, alpha=0.01, steps=1000):\n",
    "    m = y.shape[0]\n",
    "    x = np.append(np.ones((m, 1)), x, axis=1)\n",
    "    n = x.shape[1]\n",
    "    \n",
    "    theta = np.zeros((n, 1))\n",
    "    \n",
    "    costs = []\n",
    "    for _ in range(steps):\n",
    "        theta = gradient_descent(x, y, theta, alpha)\n",
    "        cost = cost_function(x, y, theta)\n",
    "        costs.append(cost)\n",
    "    return {'costs': costs, 'theta': theta}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/ex2data1.txt', header=None, names=['x1', 'x2', 'y'])\n",
    "\n",
    "x = df[['x1', 'x2']].to_numpy()\n",
    "y = df[['y']].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['x1'], df['x2'], c=df['y']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.0009\n",
    "steps = 200000\n",
    "\n",
    "result = fit(x, y, alpha=alpha, steps=steps)\n",
    "\n",
    "costs, theta = result['costs'], result['theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('theta_0 = %s'%theta[0, 0], 'theta_1 = %s'%theta[1, 0], 'theta_2 = %s'%theta[2, 0], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Test\n",
    "\n",
    "Cost values should always decrease on every itteration, if not then we may have to adjust (lower) our learning rate, alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cost(costs):\n",
    "    \"\"\"\n",
    "    Checks if the cost is always decreasing.\n",
    "    \"\"\"\n",
    "    for i in range(len(costs)-1):\n",
    "        if costs[i] < costs[i+1]:\n",
    "            print('error: cost increased at step: {}\\nDecrease your learning rate...'.format(i+1))\n",
    "            return \n",
    "        \n",
    "    print(\"alpha seems to be low enough for gradient descent algorithm to converge to minimum.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cost(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost by itterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the hypothesis (Boundry) line\n",
    "\n",
    "Since we only have two features $x_1$ and $x_2$, we can plot one as a function of another on a 2D graph:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$$\n",
    "\n",
    "We want to plot $x_2$ as a function of $x_1$, hence rearranging for $x_2$, we get:\n",
    "\n",
    "$$x_2 = -\\frac{1}{\\theta_2}(\\theta_0 + \\theta_1x_1)$$\n",
    "\n",
    "Let $\\theta \\setminus \\theta_2$ be a column vector with all values of $\\theta$ except $\\theta_2$, then the Vectorised form is:\n",
    "\n",
    "$$x_2 = -\\frac{1}{\\theta_2}X\\theta \\setminus \\theta_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, theta):\n",
    "    x1_plot = np.linspace(x[:, 0].min(), x[:, 1].max())[:, np.newaxis]\n",
    "    ones = np.ones((x1_plot.shape[0], 1))\n",
    "\n",
    "    x1 = np.append(ones, x1_plot, axis=1)\n",
    "    x2_plot = -(1/theta[2, 0]) * x1 @ theta[:2, :]\n",
    "\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y);\n",
    "    plt.plot(x1_plot[:, 0], x2_plot[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy\n",
    "\n",
    "We need to convert the probabilities into binary values to get y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "  y =\n",
    "    \\begin{cases}\n",
    "      0 & \\text{if $h_{\\theta}(x) <= 0.5$}\\\\\n",
    "      1 & \\text{otherwise}\n",
    "    \\end{cases}       \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(x, y, theta, add_ones=True):\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    \n",
    "    if add_ones:\n",
    "        x = np.append(np.ones((m, 1)), x, axis=1)\n",
    "        \n",
    "    h = sigmoid_function(x @ theta)\n",
    "    \n",
    "    y_pred = np.round(h)\n",
    "\n",
    "    return (y_pred == y).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Accuracy: {}%'.format(get_accuracy(x, y, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Sklearn\n",
    "\n",
    "This serves as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x, y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.append(model.intercept_, model.coef_)[:, np.newaxis]\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Gradient Descent\n",
    "\n",
    "We will implement [feature scaling](https://www.coursera.org/learn/machine-learning/supplement/CTA0D/gradient-descent-in-practice-i-feature-scaling) so that the theta values converge faster to global minimum in our gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Gradient Descent converges faster to global minimum if we scale our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimum theta values\n",
    "\n",
    "We know the optimium theta values from running Logistic Regression on Sklearn:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta \\approx\n",
    "    \\begin{bmatrix}\n",
    "    -25.05219314\\\\\n",
    "    0.20535491\\\\\n",
    "    0.2005838\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theta values from our implementation\n",
    "\n",
    "We can see that the theta values in our implementation are converging towards the optimum theta values. However, after 200,000 steps, our values are still not close enough to the optimum:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta \\approx\n",
    "    \\begin{bmatrix}\n",
    "-7.0102\\\\\n",
    "0.0621\\\\\n",
    "0.0555\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need features' values $x_i$ to be approximetely between -1 and 1 i.e. $-1 <=x_{i} <= 1$, but not too small either. Ideally, we would like:\n",
    "\n",
    "$$0.3 <= |x_{i}| <= 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Normalization\n",
    "\n",
    "We can use mean normalization to get feature values on the same scale:\n",
    "\n",
    "$$ x_{i} := \\frac{x_{i} - \\mu_{i}}{s_{i}},$$\n",
    "\n",
    "where $\\mu_i$ is the mean of all values of feature i, and $s_{i}$ is range of values (max - min) or standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(x):\n",
    "    \"\"\"\n",
    "    Normalizes all feature values.\n",
    "    \"\"\"\n",
    "    means = x.mean(axis=0)\n",
    "    stds = x.std(axis=0)\n",
    "    \n",
    "    for i in range(x.shape[1]):\n",
    "        x[:, i] = (x[:, i] - means[i])/stds[i]\n",
    "        \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y, alpha, steps=100):\n",
    "\n",
    "    # Normalize features\n",
    "    x = feature_normalize(x)\n",
    "\n",
    "    m = y.shape[0]\n",
    "    x = np.append(np.ones((m, 1)), x, axis=1)\n",
    "    n = x.shape[1]\n",
    "    \n",
    "    theta = np.zeros((n, 1))\n",
    "    \n",
    "    costs = []\n",
    "    for _ in range(steps):\n",
    "        theta = gradient_descent(x, y, theta, alpha)\n",
    "        cost = cost_function(x, y, theta)\n",
    "        costs.append(cost)\n",
    "    return {'costs': costs, 'theta': theta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "steps = 50000\n",
    "\n",
    "result = fit(x, y, alpha=alpha, steps=steps)\n",
    "\n",
    "costs, theta = result['costs'], result['theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cost(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('theta_0 = %s'%theta[0, 0], 'theta_1 = %s'%theta[1, 0], 'theta_2 = %s'%theta[2, 0], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Accuracy: {}%'.format(get_accuracy(x, y, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check theta values using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x, y[:, 0])\n",
    "theta = np.append(model.intercept_, model.coef_)[:, np.newaxis]\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sklearn model accuracy: {:.0%}'.format(model.score(x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression\n",
    "\n",
    "[Overfitting](https://www.coursera.org/learn/machine-learning/supplement/VTe37/the-problem-of-overfitting) can occur if there are too many features in our dataset. One way to deal with overfitting is to use [regularization](https://www.coursera.org/learn/machine-learning/supplement/v51eg/regularized-logistic-regression).\n",
    "\n",
    "\n",
    "#### Lasso (L1) and Ridge (L2) Regularization Methods\n",
    "https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "\n",
    "\n",
    "We will be implementing L2 Regularization method, also known as Ridge Regression.\n",
    "\n",
    "\n",
    "#### Normalizing for Regularization\n",
    "Features are usually [normalized](https://stats.stackexchange.com/a/195391) before applying Lasso or Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Regularization\n",
    "To implement regularization, we must add the regularization term to:\n",
    "- **Cost function**\n",
    "- **Gradient Descent Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/supplement/1tJlY/cost-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function with Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/regularized-cost-function.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_L2(x, y, theta, l):\n",
    "    \"\"\"\n",
    "    Returns the cost for given x, y, and theta.\n",
    "    \n",
    "    Params:\n",
    "        x (numpy array): ((M x (N+1)) matrix), these are \n",
    "            the dataset features.\n",
    "            \n",
    "        y (numpy array): (M x 1) column vector, this is\n",
    "            the value to be predicted.\n",
    "            \n",
    "        theta (numpy array): Co-efficients\n",
    "        \n",
    "        l (float): Regularization term for Ridge regression\n",
    "    \n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    z = x @ theta\n",
    "    h = sigmoid_function(z)\n",
    "    \n",
    "    c1 = -y.T @ np.log(h)\n",
    "    c2 = (1-y).T @ np.log(1-h)\n",
    "    \n",
    "    J = (1/m) * ( c1 - c2 )\n",
    "    \n",
    "    regularization_term = ( l/(2*m) ) * theta[1:, :].sum()\n",
    "    \n",
    "    J = J[0, 0] + regularization_term\n",
    "    \n",
    "    return J\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Gradient Descent Algorithm\n",
    "\n",
    "The derivative part of Gradient Descent algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/regularized-gradient-descent-j0.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/regularized-gradient-descent-jn.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorised implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_0 := \\theta_0 - \\frac{\\alpha}{m} X_1^T(g(X\\theta) - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_j := \\theta_j(1 - \\alpha \\frac{\\lambda}{m}) - \\frac{\\alpha}{m} X_j^T(g(X\\theta) - y),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $X_1$ is a column vector with the first column of X and $X_j$ is a matrix with all columns of X except the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_L2(x, y, theta, alpha, l):\n",
    "    \"\"\"\n",
    "    Returns coefficients (theta) by performing one step\n",
    "    of gradient descent.\n",
    "    \n",
    "    Params:\n",
    "        x (numpy array): ((M x (N+1)) matrix), these are \n",
    "            the dataset features.\n",
    "            \n",
    "        y (numpy array): (M x 1) column vector, this is\n",
    "            the value to be predicted.\n",
    "            \n",
    "        theta (numpy array): Co-efficients\n",
    "        \n",
    "        alpha (float): Learning rate for gradient descent\n",
    "            algorithm\n",
    "    \n",
    "        l (float): lambda value for regularization\n",
    "        \n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    z = x @ theta\n",
    "    h = sigmoid_function(z)\n",
    "    errors = h - y\n",
    "    \n",
    "    theta_0 = theta[0, 0].copy()\n",
    "    theta_j = theta[1:, :].copy()\n",
    "    \n",
    "    # No regularization for theta_0\n",
    "    theta_0 = theta_0 - ((alpha/m) * (x[:, 0][:, np.newaxis].T @ errors))\n",
    "    \n",
    "    # regulartization term\n",
    "    r = 1 - (alpha * l/m)\n",
    "    \n",
    "    tmp_derivative = ( (alpha/m) * (x[:, 1:].T @ errors) )\n",
    "    \n",
    "    # Update theta_1, theta_2, ..., theta_n\n",
    "    theta_j = (theta_j * r) - tmp_derivative\n",
    "    \n",
    "    # append theta0 and thetaj values\n",
    "    theta = np.append(theta_0, theta_j, axis=0)\n",
    "    \n",
    "    return theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_L2(x, y, alpha, l, steps=100):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    x = feature_normalize(x)\n",
    "    x = np.append(np.ones((m, 1)), x, axis=1)\n",
    "    n = x.shape[1]\n",
    "    \n",
    "    theta = np.zeros((n, 1))\n",
    "    \n",
    "    costs = []\n",
    "    for _ in range(steps):\n",
    "        theta = gradient_descent_L2(x, y, theta, alpha, l)\n",
    "        cost = cost_function_L2(x, y, theta, l)\n",
    "        costs.append(cost)\n",
    "        \n",
    "    return {'costs': costs, 'theta': theta}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/ex2data2.txt', header=None, names=['x1', 'x2', 'y'])\n",
    "\n",
    "x = df[['x1', 'x2']].to_numpy()\n",
    "y = df[['y']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['x1'], df['x2'], c=df['y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a straight line will not be a good fit for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping ([p.8](./misc/machine-learning-ex2/ex2.pdf))\n",
    "Creating more features is one way to fit the data better. We will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features(x1, x2, degree=6):\n",
    "    \"\"\"\n",
    "    Maps the features into all polynomial terms\n",
    "    up to the specified degree.\n",
    "    \n",
    "    Params:\n",
    "        x1 (np array): mx1 numpy array, feature in dataset\n",
    "        x2 (np array): mx1 numpy array, feature in dataset\n",
    "        \n",
    "    e.g. \n",
    "        degree = 6 will return and array of size\n",
    "        (m x 49) where m is the number of rows.\n",
    "        Powers for x1 x2\n",
    "        ---\n",
    "        x1 exponents -> 0 1 2 3 4 5 6\n",
    "        x2 exponents -> 0 1 2 3 4 5 6\n",
    "        ---\n",
    "        00 01 02 ... 06\n",
    "        10 11 12 ... 16\n",
    "            ...\n",
    "        60 61 62 ... 66\n",
    "        \n",
    "    \"\"\"\n",
    "    result = np.ones((x1.shape[0], 1))\n",
    "    for i in range(0, degree+1):\n",
    "        for j in range(0, degree+1):\n",
    "            if i == j == 0:\n",
    "                continue\n",
    "            v = (x1**i) * (x2**j)\n",
    "            result = np.hstack((result, v))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add feature mapping to fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_L2(x, y, alpha, l, steps=100):\n",
    "    \"\"\"\n",
    "    Regularized Logistic Regression (Ridge).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Feature mapping\n",
    "    x1, x2 = x[:, 0][:, np.newaxis], x[:, 1][:, np.newaxis]\n",
    "    x = map_features(x1, x2, degree=6)\n",
    "    \n",
    "    # Normalize features for faster convergence\n",
    "    x = feature_normalize(x[:, 1:])\n",
    "    x = np.append(np.ones((m, 1)), x, axis=1)\n",
    "\n",
    "    n = x.shape[1]\n",
    "    \n",
    "    theta = np.zeros((n, 1))\n",
    "    \n",
    "    costs = []\n",
    "    for _ in range(steps):\n",
    "        theta = gradient_descent_L2(x, y, theta, alpha, l)\n",
    "        cost = cost_function_L2(x, y, theta, l)\n",
    "        costs.append(cost)\n",
    "    \n",
    "    print(x.shape, y.shape, theta.shape)\n",
    "    accuracy = get_accuracy(x, y, theta, add_ones=False)\n",
    "    \n",
    "    return {'costs': costs, 'theta': theta, 'accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0004\n",
    "l = 10\n",
    "steps = 3000\n",
    "\n",
    "result = fit_L2(x, y, alpha=alpha, l=10, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs, theta = result['costs'], result['theta']\n",
    "\n",
    "check_cost(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(theta, columns=['theta']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Accuracy: {}%'.format(get_accuracy(x, y, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "Feature mapping is susceptable to overfitting. Lets see what happens when we do not use regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "\n",
    "#### Regularized Linear Regression\n",
    "https://www.coursera.org/learn/machine-learning/supplement/pKAsc/regularized-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 10e-12\n",
    "steps = 10000\n",
    "l = 10\n",
    "\n",
    "result = fit_L2(x, y, alpha=alpha, l=l, steps=steps)\n",
    "costs, theta = result['costs'], result['theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('theta_0 = %s'%theta[0, 0], 'theta_1 = %s'%theta[1, 0], 'theta_2 = %s'%theta[2, 0], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cost(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(x, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Accuracy: {}%'.format(get_accuracy(x, y, theta)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
